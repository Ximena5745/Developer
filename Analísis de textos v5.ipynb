{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxMe9GkKkqx7sRcy0VMG9l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ximena5745/Developer/blob/main/Anal%C3%ADsis%20de%20textos%20v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter unidecode textblob plotly nltk\n",
        "import nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAG7RV9TiyJ9",
        "outputId": "f33a6c0d-bae7-4a52-9f8f-5f2a0b0de19d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.24.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (24.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter, unidecode\n",
            "Successfully installed unidecode-1.3.8 xlsxwriter-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Q14Y7AnjDv_",
        "outputId": "0634016a-4da7-40b1-9f2b-8c7febd361c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "from typing import List, Tuple, Dict\n",
        "from difflib import SequenceMatcher\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "# Descargar las stopwords de nltk si no están disponibles\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VduLVgFzH2D",
        "outputId": "5808f077-f17b-4f97-92e5-f958f571f63e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CalidadAnalyzerPDI:\n",
        "    def __init__(self, archivo_excel: str):\n",
        "        \"\"\"Inicializa el analizador\"\"\"\n",
        "        self.archivo_excel = archivo_excel\n",
        "        self.min_longitud_frase = 4\n",
        "        self.similitud_umbral = 0.75\n",
        "        self.df_calidad = self.cargar_datos()\n",
        "        self.stopwords_es = stopwords.words(\"spanish\")  # Lista de stopwords en español\n",
        "        self.modelo_embeddings = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")  # Modelo semántico\n",
        "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        self.columnas_relevantes = [\n",
        "            \"Calidad\", \"Calidad2\", \"Expansión\", \"Expansión2\",\n",
        "            \"Educación para toda la vida\", \"Educación para toda la vida2\",\n",
        "            \"Experiencia\", \"Experiencia2\",\n",
        "            \"Transformación Organizacional\", \"Transformación Organizacional2\",\n",
        "            \"Sostenibilidad\", \"Sostenibilidad2\"\n",
        "        ]\n",
        "\n",
        "    def cargar_datos(self):\n",
        "        \"\"\"Carga los datos desde un archivo Excel\"\"\"\n",
        "        try:\n",
        "            df = pd.read_excel(self.archivo_excel, engine='openpyxl')\n",
        "            print(f\"✓ Archivo cargado exitosamente con {len(df)} filas y {len(df.columns)} columnas\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"✘ Error cargando datos: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _procesar_texto(self, texto: str) -> str:\n",
        "        \"\"\"Limpia y normaliza un texto\"\"\"\n",
        "        texto = unidecode(str(texto).lower())\n",
        "        texto = re.sub(r\"[^a-z\\s]\", \" \", texto)\n",
        "        return \" \".join(texto.split())\n",
        "\n",
        "    def _extraer_frases_validas(self, texto: str) -> List[str]:\n",
        "        \"\"\"Extrae frases relevantes de un texto\"\"\"\n",
        "        frases = re.split(r'[.,;]', texto)\n",
        "        return [\n",
        "            frase.strip() for frase in frases\n",
        "            if len(frase.split()) >= self.min_longitud_frase\n",
        "        ]\n",
        "\n",
        "    def _agrupar_frases_por_contexto(self, frases: List[str]) -> List[str]:\n",
        "        \"\"\"Agrupa frases similares semánticamente\"\"\"\n",
        "        embeddings = self.modelo_embeddings.encode(frases, convert_to_tensor=True)\n",
        "        similitudes = cosine_similarity(embeddings)\n",
        "\n",
        "        grupos = defaultdict(list)\n",
        "        procesadas = set()\n",
        "\n",
        "        for i, frase in enumerate(frases):\n",
        "            if i in procesadas:\n",
        "                continue\n",
        "            grupo_actual = [i]\n",
        "            for j in range(i + 1, len(frases)):\n",
        "                if j not in procesadas and similitudes[i, j] > self.similitud_umbral:\n",
        "                    grupo_actual.append(j)\n",
        "                    procesadas.add(j)\n",
        "\n",
        "            # Seleccionar frase representativa del grupo\n",
        "            frase_representativa = max(grupo_actual, key=lambda idx: len(frases[idx]))\n",
        "            grupos[frase_representativa].extend(grupo_actual)\n",
        "\n",
        "        # Construir resultado con las mejores frases\n",
        "        frases_representativas = [frases[key] for key in grupos.keys()]\n",
        "        return frases_representativas, grupos\n",
        "\n",
        "    def _contar_ocurrencias(self, textos: List[str], grupos: dict, frases: List[str]) -> List[Tuple[str, int]]:\n",
        "        \"\"\"Cuenta todas las ocurrencias explícitas e implícitas de los grupos en los comentarios\"\"\"\n",
        "        conteos = defaultdict(int)\n",
        "        embeddings_grupos = self.modelo_embeddings.encode(frases, convert_to_tensor=True)\n",
        "\n",
        "        for texto in textos:\n",
        "            texto_limpio = self._procesar_texto(texto)\n",
        "            frases_comentario = self._extraer_frases_validas(texto_limpio)\n",
        "            if not frases_comentario:\n",
        "                continue\n",
        "\n",
        "            embeddings_comentario = self.modelo_embeddings.encode(frases_comentario, convert_to_tensor=True)\n",
        "            similitudes = cosine_similarity(embeddings_comentario, embeddings_grupos)\n",
        "\n",
        "            for i, frase in enumerate(frases_comentario):\n",
        "                # Identificar el grupo más similar\n",
        "                grupo_idx = np.argmax(similitudes[i])\n",
        "                if similitudes[i, grupo_idx] > self.similitud_umbral:\n",
        "                    conteos[frases[grupo_idx]] += 1\n",
        "\n",
        "        # Consolidar frecuencias finales\n",
        "        resultado = [(frase, conteos[frase]) for frase in frases]\n",
        "        resultado.sort(key=lambda x: x[1], reverse=True)\n",
        "        return resultado\n",
        "\n",
        "    def analizar_columna(self, columna: str) -> List[Tuple[str, int]]:\n",
        "        \"\"\"Analiza una columna de comentarios y genera las frases más relevantes\"\"\"\n",
        "        if columna not in self.df_calidad.columns:\n",
        "            print(f\"⚠️ Columna {columna} no encontrada en el archivo.\")\n",
        "            return []\n",
        "\n",
        "        print(f\"Analizando columna: {columna}\")\n",
        "        textos = self.df_calidad[columna].dropna().astype(str).tolist()\n",
        "        frases_totales = []\n",
        "        for texto in textos:\n",
        "            texto_limpio = self._procesar_texto(texto)\n",
        "            frases_totales.extend(self._extraer_frases_validas(texto_limpio))\n",
        "\n",
        "        if not frases_totales:\n",
        "            print(f\"⚠️ No se encontraron frases válidas en la columna {columna}.\")\n",
        "            return []\n",
        "\n",
        "        frases_unicas = list(set(frases_totales))\n",
        "        frases_representativas, grupos = self._agrupar_frases_por_contexto(frases_unicas)\n",
        "        resultado = self._contar_ocurrencias(textos, grupos, frases_representativas)\n",
        "\n",
        "        return resultado\n",
        "\n",
        "    def generar_reporte_excel(self) -> str:\n",
        "        \"\"\"Genera un archivo Excel con los resultados del análisis\"\"\"\n",
        "        resultados = {}\n",
        "        for columna in self.columnas_relevantes:\n",
        "            if columna in self.df_calidad.columns:\n",
        "                resultados[columna] = self.analizar_columna(columna)\n",
        "            else:\n",
        "                print(f\"⚠️ Columna {columna} no encontrada en el archivo.\")\n",
        "\n",
        "        nombre_archivo = f\"analisis_calidad_{self.timestamp}.xlsx\"\n",
        "        with pd.ExcelWriter(nombre_archivo, engine=\"xlsxwriter\") as writer:\n",
        "            for columna, frases in resultados.items():\n",
        "                if frases:\n",
        "                    df_resultados = pd.DataFrame(frases, columns=[\"Frase\", \"Frecuencia\"])\n",
        "                    df_resultados.to_excel(writer, sheet_name=columna[:31], index=False)\n",
        "\n",
        "        print(f\"✓ Análisis completado: {nombre_archivo}\")\n",
        "        return nombre_archivo\n"
      ],
      "metadata": {
        "id": "Exz3g72wAN7-"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ejecutar_analisis(archivo_excel: str):\n",
        "    \"\"\"Función principal para ejecutar el análisis\"\"\"\n",
        "    try:\n",
        "        analyzer = CalidadAnalyzerPDI(archivo_excel)\n",
        "        reporte = analyzer.generar_reporte_excel()\n",
        "        print(f\"✓ Reporte generado: {reporte}\")\n",
        "    except Exception as e:\n",
        "        print(f\"✘ Error durante el análisis: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    archivo = \"Preguntas abiertas 19_11_2024.xlsx\"  # Cambiar por el nombre correcto del archivo\n",
        "    ejecutar_analisis(archivo)"
      ],
      "metadata": {
        "id": "PxpWHXixATOq",
        "outputId": "4da64d66-8fff-4153-efd0-0b68853e3535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Archivo cargado exitosamente con 3777 filas y 18 columnas\n",
            "Analizando columna: Calidad\n",
            "Analizando columna: Calidad2\n",
            "Analizando columna: Expansión\n",
            "Analizando columna: Expansión2\n",
            "Analizando columna: Educación para toda la vida\n",
            "Analizando columna: Educación para toda la vida2\n",
            "Analizando columna: Experiencia\n",
            "Analizando columna: Experiencia2\n",
            "Analizando columna: Transformación Organizacional\n",
            "Analizando columna: Transformación Organizacional2\n",
            "Analizando columna: Sostenibilidad\n",
            "Analizando columna: Sostenibilidad2\n",
            "✓ Análisis completado: analisis_calidad_20241124_070946.xlsx\n",
            "✓ Reporte generado: analisis_calidad_20241124_070946.xlsx\n"
          ]
        }
      ]
    }
  ]
}